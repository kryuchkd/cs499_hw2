{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Overview\n",
    "\n",
    "In this assignment, we will study language model. You will get the basic ideas of maximum likelihood estimation, smoothing, generate text documents from language models, and language model evaluation. \n",
    "\n",
    "We will reuse the same Yelp dataset and refer to each individual user review as a **document** (e.g., as in computing document frequency). You should reuse your JSON parser in this assignment.\n",
    "\n",
    "The same pre-processing steps you have developed in HW1 will be used in this assignment, i.e., tokenization, stemming and normalization. Note: **NO** stopword removal is needed in this assignment. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Statistical Language Models\n",
    "\n",
    "### 1. Maximum likelihood estimation for statistical language models with proper smoothing (50pts)\n",
    "\n",
    "Use all the review documents to estimate a unigram language model $p(w)$ and two bigram language models (with different smoothing methods specified below). Note those language models are corpus-level models, i.e., aggregating all the words across different documents.\n",
    "\n",
    "When estimating the bigram language models, using linear interpolation smoothing and absolute discount smoothing based on the unigram language model $p_u(w)$ to get two different bigram language models accordingly, i.e., $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$. In linear interpolation smoothing, set the parameter $\\lambda=0.9$; and in absolute discount smoothing, set the parameter $\\delta=0.1$.\n",
    "\n",
    "Specifically, when estimating $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$, you should use the unigram language model $p(w_i)$ as the reference language model in smoothing. For example, in linear interpolation smoothing, the resulting smoothing formula looks like this,\n",
    "\n",
    "$$p^L(w_i|w_{i-1})=(1-\\lambda) \\frac{c(w_{i-1}w_i)}{c(w_{i-1})} + \\lambda p(w_i)$$ \n",
    "where $c(w_{i-1}w_i)$ is the frequency of bigram $w_{i-1}w_i$ in the whole corpus.\n",
    "\n",
    "From the resulting two bigram language models, find the top 10 words that are most likely to follow the word \"good\", i.e., rank the words in a descending order by $p^L(w|good\")$ and $p^A(w|good\")$ and output the top 10 words. Are those top 10 words the same from these two bigram language models? Explain your observation.\n",
    "\n",
    "*HINT: to reduce space complexity, you do not need to actually maintain a $V\\times V$ array to store the counts and probabilities for the bigram language models. You can use a sparse data structure, e.g., hash map, to store the seen words/bigrams, and perform the smoothing on the fly, i.e., evoke some function calls to return the value of $p^L(w|good\")$ and $p^A(w|good\")$.* \n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of the linear interpolation smoothing and absolute discount smoothing.\n",
    "2. The top 10 words selected from the corresponding two bigram language models.\n",
    "3. Your explanation of the observations about the top words under those two bigram language models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram language model p(w)\n",
    "list_of_tokenized_reviews = []\n",
    "with open('list_of_tokenized_reviews.pickle', 'rb') as file:\n",
    "    list_of_tokenized_reviews = pickle.load(file)\n",
    "token_freq_dict = total_term_frequency(list_of_tokenized_reviews)\n",
    "total_num_tokens = sum([len(tokens) for tokens in list_of_tokenized_reviews])\n",
    "token_prob_dict = {}\n",
    "for key in token_freq_dict.keys():\n",
    "    token_prob_dict[key] = token_freq_dict[key] / total_num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 0.05306509260194806\n",
      "and : 0.03497929899421944\n",
      "i : 0.03195698201920177\n",
      "a : 0.02365215008387146\n",
      "to : 0.019990190189529227\n",
      "but : 0.01815881364406983\n",
      "it : 0.017903926404976605\n",
      "wa : 0.01704291139500162\n",
      "of : 0.014552406250610416\n",
      "for : 0.011796944880580106\n"
     ]
    }
   ],
   "source": [
    "#bigram language model with linear interpolation smoothing\n",
    "all_bigrams = get_all_bigrams(list_of_tokenized_reviews)\n",
    "bigram_freq_dict = total_bigram_freqency(all_bigrams)\n",
    "def p_linear(first, second):\n",
    "    lam = 0.9\n",
    "    try:\n",
    "        prob = (1-lam) * (bigram_freq_dict[(first, second)] / token_freq_dict[first]) + (lam * token_prob_dict[second])\n",
    "    except:\n",
    "        prob = (lam * token_prob_dict[second])\n",
    "    return prob\n",
    "#get the top most likely words to follow \"good\"\n",
    "linear_probabilities = {}\n",
    "for token in token_prob_dict.keys():\n",
    "    linear_probabilities[token] = p_linear('good', token)\n",
    "ans = dict(sorted(linear_probabilities.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('good', 'but') : 7560\n",
      "('good', 'and') : 5022\n",
      "('good', 'i') : 4455\n",
      "('good', 'the') : 4358\n",
      "('good', 'as') : 2874\n",
      "('good', 'food') : 2306\n",
      "('good', 'it') : 1507\n",
      "('good', 'thing') : 1403\n",
      "('good', 'for') : 1393\n",
      "('good', 'too') : 1385\n"
     ]
    }
   ],
   "source": [
    "#most popular words to follow 'good' with no smooting.\n",
    "target_bigrams = {}\n",
    "for bigram in bigram_freq_dict.keys():\n",
    "    if bigram[0] == 'good':\n",
    "        target_bigrams[bigram] = bigram_freq_dict[bigram]\n",
    "ans = dict(sorted(target_bigrams.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but : 0.09564301951999492\n",
      "and : 0.063624517412393\n",
      "i : 0.05644462810001931\n",
      "the : 0.05530186422700514\n",
      "as : 0.03636073506188902\n",
      "food : 0.029181046543558607\n",
      "it : 0.019120838648309255\n",
      "thing : 0.017747196904450483\n",
      "for : 0.01765550959445897\n",
      "too : 0.01752179260050426\n"
     ]
    }
   ],
   "source": [
    "#bigram language model with absolute discount smoothing\n",
    "def p_abs_disc(first, second):\n",
    "    delta = 0.1\n",
    "    d_u = len(target_bigrams) #number of unique bigrams with 'good' as the first word\n",
    "    try:\n",
    "        prob = (max(bigram_freq_dict[(first, second)] - delta, 0) + (delta * d_u * token_prob_dict[second])) / (token_freq_dict[first])\n",
    "    except:\n",
    "        prob = (delta * d_u * token_prob_dict[second]) / (token_freq_dict[first])\n",
    "    return prob\n",
    "\n",
    "abs_disc_probabilities = {}\n",
    "for token in token_prob_dict.keys():\n",
    "    abs_disc_probabilities[token] = p_abs_disc('good', token)\n",
    "ans = dict(sorted(abs_disc_probabilities.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate text documents from a language model (40pts)\n",
    "\n",
    "Fixing the document length to 20, generate 10 documents by sampling words from $p(w)$, $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$ respectively.\n",
    "\n",
    "*HINT: you can use $p(w)$ to generate the first word of a document and then sampling from the corresponding bigram language model when generating from $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$.* \n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of the sampling procedure from a language model.\n",
    "2. The 10 documents generated from $p(w)$, $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$ accordingly, and the corresponding likelihood given by the used language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "doc_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9999999999991019\n",
      "1. sauc i best glace to burger too equi up drool besid peopl the warm too s die had i are\n",
      "2. one check tapa singl green trader beer the one too NUM star is eateri too d mani methink lou of\n",
      "3. that here shock up of just front huge and we for again to the more have aot you if take\n",
      "4. told background valu dessert anyth is i like it did and to a wallet super mouth tasti fill the made\n",
      "5. the dish larger i which damn crazi home i the experi take order i were my see anyth be egg\n",
      "6. one for go after and delici a everyth we up the friend littl wa were dine one it wateri nt\n",
      "7. over i chef the big not make for expect for bad two chicago boggl is everi anymor be far dinner\n",
      "8. la a your the joke finish with wa both disappoint of nt for s flaki ringer smallish the head and\n",
      "9. great want food in eaten NUM about go it alway howev were i idea time for atmospher just here cajeta\n",
      "10. tomato in by grill healthiest realli after ridicul the twice walk record it side chop and do new and a\n"
     ]
    }
   ],
   "source": [
    "#unigram\n",
    "unigram_tokens = []\n",
    "unigram_probs = []\n",
    "for key in token_prob_dict.keys():\n",
    "    unigram_tokens.append(key)\n",
    "    unigram_probs.append(token_prob_dict[key])\n",
    "print(sum(token_prob_dict.values()))\n",
    "for i in range(10):\n",
    "    samples = np.random.choice(unigram_tokens, doc_size, p=unigram_probs)\n",
    "    print(str(i+1) +'. ' + ' '.join(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19. bar sardin tell might wa with be enjoy stuf are great offer were at three cover dream it heavi our\n",
      "19. seriou not ball consid of food your i highli bit disappoint amaz check ahead with they brunt and if sometim\n",
      "19. there these be and and wa ha fish so much nt that warm s for and mixologist who tri over\n",
      "19. they chicken hi chees out burger even befor a eat onc fantast cup sassi thi saw loud vegetarian hearth on\n",
      "19. first chicken we provolon winner of check bacon tabl abl realli but describ dine NUM chicken hostess here serv includ\n",
      "19. beef get the here pitcher noodl tri the dish for pastrami bread of messi comfort some spoke group NUM of\n",
      "19. for so i it side also boston good you make line sandwich and there by cook wa are look my\n",
      "19. quick they were oh NUM downtown week i felt throughout favorit half an venu and ve tri onc the would\n",
      "19. were wa at who sat back slice still what enjoy dish hi mushroom busi creation hous the of order ve\n",
      "19. deafen the instead do environ with day is next are is that what pleas the to be tri meatbal canter\n"
     ]
    }
   ],
   "source": [
    "#bigram linear interp\n",
    "\n",
    "for i in range(10):\n",
    "    samples = []\n",
    "    prev_word = np.random.choice(unigram_tokens, 1, p=unigram_probs)[0]\n",
    "    samples.append(prev_word)\n",
    "    for i in range(doc_size-1):\n",
    "        lin_int_probs_dict = {}\n",
    "        for token in token_prob_dict.keys():\n",
    "            lin_int_probs_dict[token] = p_linear(prev_word, token)\n",
    "        lin_int_tokens = []\n",
    "        lin_int_probs = []\n",
    "        for key in lin_int_probs_dict.keys():\n",
    "            lin_int_tokens.append(key)\n",
    "            lin_int_probs.append(lin_int_probs_dict[key])\n",
    "        lin_int_probs = np.array(lin_int_probs)\n",
    "        lin_int_probs /= sum(lin_int_probs)\n",
    "        prev_word = np.random.choice(lin_int_tokens, 1, p=lin_int_probs)[0]\n",
    "        samples.append(prev_word)\n",
    "    print(str(i+1) +'. ' + ' '.join(samples))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we :0.9999976292062261\n",
      "had :0.9996919785071037\n",
      "our :0.999986439143738\n",
      "veri :0.9999976241368265\n",
      "last :0.999819840106166\n",
      "meal :0.9967054486790975\n",
      "in :0.9998599954237022\n",
      "n :0.9998785916613644\n",
      "o :0.9960159362530482\n",
      "here :0.9973157850945462\n",
      "and :0.9999995446495747\n",
      "it :0.9987296750018573\n",
      "wa :0.9999691277013354\n",
      "great :0.9988274908284445\n",
      "their :0.9999858075484614\n",
      "sweet :0.9994916161236482\n",
      "tea :0.9982686567144848\n",
      "nice :0.9989399675480932\n",
      "my :0.9999962264487733\n",
      "i :0.9999955429638958\n",
      "decid :0.9999284282832092\n",
      "to :0.999931987202132\n",
      "tri :0.9985445817568287\n",
      "a :0.9999795452870724\n",
      "the :0.9999987664252796\n",
      "craw :0.999999999998067\n",
      "fish :0.9996218970883187\n",
      "pie :0.9977388439633261\n",
      "with :0.9999545675788161\n",
      "side :0.9992028407836309\n",
      "of :0.9999859752602991\n",
      "green :0.999841605066707\n",
      "pretti :0.9999527881216603\n",
      "much :0.9992625595310232\n",
      "an :0.9999999999980835\n",
      "empanada :0.9992900608499937\n",
      "stuf :0.9993030303010977\n",
      "fill :0.9994615077762874\n",
      "similar :0.9994449583699457\n",
      "ettoufe :0.9961538461519128\n",
      "pastri :0.9985526315770143\n",
      "flaki :0.9995381062336329\n",
      "perfectli :0.9994885394941847\n",
      "bake :0.9996433348175053\n",
      "or :0.9999999999980848\n",
      "fri :0.9993227717963604\n",
      "am :0.999693867787219\n",
      "not :0.9998480036558746\n",
      "even :0.9994566353167729\n",
      "sure :0.9972138047118723\n",
      "but :0.9999977257373934\n",
      "good :0.9980308836343028\n",
      "tast :0.9994007155615732\n",
      "bit :0.9996471564945442\n",
      "friend :0.9984399033160258\n",
      "macaroni :0.9999999999980668\n",
      "chees :0.9995519669125539\n",
      "which :0.9999999999980667\n",
      "wonder :0.9986929243619562\n",
      "after :0.9999037489754014\n",
      "lunch :0.9982150184633937\n",
      "also :0.9998146196047717\n",
      "order :0.9997021565355465\n",
      "dessert :0.9981818181798844\n",
      "peanut :0.9999999999980672\n",
      "brittl :0.9999999999980667\n",
      "homemad :0.999742101867828\n",
      "ice :0.9999606144131945\n",
      "cream :0.999228286794757\n",
      "have :0.9998796399422512\n",
      "never :0.9999604743063677\n",
      "so :0.9998982424824653\n",
      "incred :0.9984454569801104\n",
      "creami :0.9997880457801782\n",
      "definit :0.9999163617335626\n",
      "complet :0.9998188952591605\n",
      "opposit :0.9995145631048632\n",
      "out :0.9979486087431009\n",
      "carton :0.9999999999980667\n",
      "love :0.9995055821352357\n",
      "realiz :0.9998304126605879\n",
      "that :0.9996786825174635\n",
      "there :0.9988537859488152\n",
      "butcher :0.9984189723300827\n",
      "shop :0.9980223123712921\n",
      "locat :0.9975767366701199\n",
      "right :0.9988300584951416\n",
      "next :0.9996460746441423\n",
      "door :0.9974014724969833\n",
      "affili :0.9999999999980667\n",
      "cachon :0.9999999999980667\n",
      "stop :0.9985777175734142\n",
      "finish :0.99970149253538\n",
      "up :0.9989122327181139\n",
      "s :0.9995187014356881\n",
      "mother :0.9990730011568154\n",
      "could :0.999827674535816\n",
      "tradit :0.9996466431076078\n",
      "muffaletta :0.9999999999980667\n",
      "sandwich :0.9986958411805036\n",
      "take :0.9999550850157112\n",
      "on :0.9998337696859848\n",
      "plane :0.9989690721630158\n",
      "she :0.9999999999980688\n",
      "kind :0.9998019540514069\n",
      "enough :0.9990074441668034\n",
      "give :0.9999316764862384\n",
      "me :0.9981738362408004\n",
      "piec :0.9990872698139186\n",
      "glad :0.9999999999980668\n",
      "becaus :0.9999958861260908\n",
      "amaz :0.9972101410821868\n",
      "is :0.9998812592467389\n",
      "round :0.999272486770554\n",
      "differ :0.999378192314301\n",
      "sort :0.9998682476924006\n",
      "meat :0.9994402658717773\n",
      "top :0.9994609949722106\n",
      "sauc :0.9986935449864839\n",
      "consist :0.9993248945128352\n",
      "oliv :0.9998941798922463\n",
      "will :0.9998154439811238\n",
      "be :0.9997755714733577\n",
      "first :0.9997043102448544\n",
      "thing :0.9990504668955605\n",
      "eat :0.9986182231956634\n",
      "if :0.9999979113561029\n",
      "ever :0.9986968724920532\n",
      "make :0.999946399855136\n",
      "back :0.9903352861331746\n",
      "new :0.9997028526129651\n",
      "orlean :0.9932240781575379\n",
      "short :0.999793708095026\n",
      "thi :0.9998441799209485\n",
      "place :0.9980075579860858\n",
      "just :0.9999999999980766\n",
      "alright :0.9979651162771349\n",
      "doe :0.9997499062128974\n",
      "warrant :0.9989690721630151\n",
      "itself :0.9985260770955722\n",
      "NUM :0.9993945995594842\n",
      "wood :0.9999999999980669\n",
      "fire :0.9990059642127781\n",
      "oyster :0.998557072917419\n",
      "roast :0.9999759903942282\n",
      "allig :0.9995732574660614\n",
      "smoke :0.9999627004829613\n",
      "pork :0.9997976023645628\n",
      "rib :0.9992438176968218\n",
      "cochon :0.9978029766103986\n",
      "main :0.9999034127475838\n",
      "entre :0.9990560585609629\n",
      "cheek :0.9986332574012553\n",
      "gumbo :0.9994180841520507\n",
      "liver :0.9998447204949605\n",
      "realli :0.9998820600164198\n",
      "flavorfri :0.9999999999980669\n",
      "food :0.9981625627322542\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m temp_prob_dict \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token2 \u001b[38;5;129;01min\u001b[39;00m token_prob_dict\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m----> 4\u001b[0m     temp_prob_dict[token2] \u001b[38;5;241m=\u001b[39m \u001b[43mp_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken1\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m :\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(temp_prob_dict\u001b[38;5;241m.\u001b[39mvalues())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for token1 in token_prob_dict.keys():\n",
    "    temp_prob_dict = {}\n",
    "    for token2 in token_prob_dict.keys():\n",
    "        temp_prob_dict[token2] = p_linear(token1, token2)\n",
    "    print(f'{token1} :{sum(temp_prob_dict.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment — Belief or Bias in Information Retrieval (10pts)\n",
    "In our class, we have learned both classical and modern information retrieval evaluation methods. And their shared goal is to assess if a retrieval system can satisfy users' information need. Such an evaluation directly leads to the subsequent optimization of retrieval system, e.g., optimize the ranking for click-through rates. But should a system please its users so as to improve the metrics or should it educate the users about what is right and wrong?\n",
    "\n",
    "Let's read the paper [\"Beliefs and biases in web search\"](https://dl.acm.org/doi/10.1145/2484028.2484053), which is the best paper in SIGIR'2013. Based on the findings of this paper and current public concern/debate of the wide spread of misinformation on the web, what kind of suggestion do you want to give to Google and Bing to improve the situation? You can focus on the search evaluation, document retrieval and ranking, or any aspect related to the retrieval process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (5pts)\n",
    "\n",
    "You are encouraged to further investigate the relation between classic language model and the trending Large Language Models. How LLMs differ from unigram and bigram models we implemented? It is okay to consult LLMs for this question :\\) \n",
    "\n",
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is Feb 20 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
