{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Overview\n",
    "\n",
    "In this assignment, we will study language model. You will get the basic ideas of maximum likelihood estimation, smoothing, generate text documents from language models, and language model evaluation. \n",
    "\n",
    "We will reuse the same Yelp dataset and refer to each individual user review as a **document** (e.g., as in computing document frequency). You should reuse your JSON parser in this assignment.\n",
    "\n",
    "The same pre-processing steps you have developed in HW1 will be used in this assignment, i.e., tokenization, stemming and normalization. Note: **NO** stopword removal is needed in this assignment. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Statistical Language Models\n",
    "\n",
    "### 1. Maximum likelihood estimation for statistical language models with proper smoothing (50pts)\n",
    "\n",
    "Use all the review documents to estimate a unigram language model $p(w)$ and two bigram language models (with different smoothing methods specified below). Note those language models are corpus-level models, i.e., aggregating all the words across different documents.\n",
    "\n",
    "When estimating the bigram language models, using linear interpolation smoothing and absolute discount smoothing based on the unigram language model $p_u(w)$ to get two different bigram language models accordingly, i.e., $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$. In linear interpolation smoothing, set the parameter $\\lambda=0.9$; and in absolute discount smoothing, set the parameter $\\delta=0.1$.\n",
    "\n",
    "Specifically, when estimating $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$, you should use the unigram language model $p(w_i)$ as the reference language model in smoothing. For example, in linear interpolation smoothing, the resulting smoothing formula looks like this,\n",
    "\n",
    "$$p^L(w_i|w_{i-1})=(1-\\lambda) \\frac{c(w_{i-1}w_i)}{c(w_{i-1})} + \\lambda p(w_i)$$ \n",
    "where $c(w_{i-1}w_i)$ is the frequency of bigram $w_{i-1}w_i$ in the whole corpus.\n",
    "\n",
    "From the resulting two bigram language models, find the top 10 words that are most likely to follow the word \"good\", i.e., rank the words in a descending order by $p^L(w|good\")$ and $p^A(w|good\")$ and output the top 10 words. Are those top 10 words the same from these two bigram language models? Explain your observation.\n",
    "\n",
    "*HINT: to reduce space complexity, you do not need to actually maintain a $V\\times V$ array to store the counts and probabilities for the bigram language models. You can use a sparse data structure, e.g., hash map, to store the seen words/bigrams, and perform the smoothing on the fly, i.e., evoke some function calls to return the value of $p^L(w|good\")$ and $p^A(w|good\")$.* \n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of the linear interpolation smoothing and absolute discount smoothing.\n",
    "2. The top 10 words selected from the corresponding two bigram language models.\n",
    "3. Your explanation of the observations about the top words under those two bigram language models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram language model p(w)\n",
    "list_of_tokenized_reviews = []\n",
    "with open('list_of_tokenized_reviews.pickle', 'rb') as file:\n",
    "    list_of_tokenized_reviews = pickle.load(file)\n",
    "token_freq_dict = total_term_frequency(list_of_tokenized_reviews)\n",
    "total_num_tokens = sum([len(tokens) for tokens in list_of_tokenized_reviews])\n",
    "token_prob_dict = {}\n",
    "for key in token_freq_dict.keys():\n",
    "    token_prob_dict[key] = token_freq_dict[key] / total_num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram language model with linear interpolation smoothing\n",
    "all_bigrams = get_all_bigrams(list_of_tokenized_reviews)\n",
    "bigram_freq_dict = total_bigram_freqency(all_bigrams)\n",
    "def p_linear(first, second):\n",
    "    lam = 0.9\n",
    "    try:\n",
    "        prob = (1-lam) * (bigram_freq_dict[(first, second)] / token_freq_dict[first]) + (lam * token_prob_dict[second])\n",
    "    except:\n",
    "        prob = (lam * token_prob_dict[second])\n",
    "    return prob\n",
    "#get the top most likely words to follow \"good\"\n",
    "linear_probabilities = {}\n",
    "for token in token_prob_dict.keys():\n",
    "    linear_probabilities[token] = p_linear('good', token)\n",
    "ans = dict(sorted(linear_probabilities.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_bigrams = {}\n",
    "for bigram in bigram_freq_dict.keys():\n",
    "    if bigram[0] == 'good':\n",
    "        target_bigrams[bigram] = bigram_freq_dict[bigram]\n",
    "ans = dict(sorted(target_bigrams.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bigram language model with absolute discount smoothing\n",
    "def p_abs_disc(first, second):\n",
    "    delta = 0.1\n",
    "    d_u = 0\n",
    "    prob = (max(bigram_freq_dict[(first, second)] - delta, 0) + (delta * d_u * token_prob_dict[second])) / (token_freq_dict[first])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate text documents from a language model (40pts)\n",
    "\n",
    "Fixing the document length to 20, generate 10 documents by sampling words from $p(w)$, $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$ respectively.\n",
    "\n",
    "*HINT: you can use $p(w)$ to generate the first word of a document and then sampling from the corresponding bigram language model when generating from $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$.* \n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of the sampling procedure from a language model.\n",
    "2. The 10 documents generated from $p(w)$, $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$ accordingly, and the corresponding likelihood given by the used language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment — Belief or Bias in Information Retrieval (10pts)\n",
    "In our class, we have learned both classical and modern information retrieval evaluation methods. And their shared goal is to assess if a retrieval system can satisfy users' information need. Such an evaluation directly leads to the subsequent optimization of retrieval system, e.g., optimize the ranking for click-through rates. But should a system please its users so as to improve the metrics or should it educate the users about what is right and wrong?\n",
    "\n",
    "Let's read the paper [\"Beliefs and biases in web search\"](https://dl.acm.org/doi/10.1145/2484028.2484053), which is the best paper in SIGIR'2013. Based on the findings of this paper and current public concern/debate of the wide spread of misinformation on the web, what kind of suggestion do you want to give to Google and Bing to improve the situation? You can focus on the search evaluation, document retrieval and ranking, or any aspect related to the retrieval process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (5pts)\n",
    "\n",
    "You are encouraged to further investigate the relation between classic language model and the trending Large Language Models. How LLMs differ from unigram and bigram models we implemented? It is okay to consult LLMs for this question :\\) \n",
    "\n",
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is Feb 20 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
