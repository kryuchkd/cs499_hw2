{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW2 Overview\n",
    "\n",
    "In this assignment, we will study language model. You will get the basic ideas of maximum likelihood estimation, smoothing, generate text documents from language models, and language model evaluation. \n",
    "\n",
    "We will reuse the same Yelp dataset and refer to each individual user review as a **document** (e.g., as in computing document frequency). You should reuse your JSON parser in this assignment.\n",
    "\n",
    "The same pre-processing steps you have developed in HW1 will be used in this assignment, i.e., tokenization, stemming and normalization. Note: **NO** stopword removal is needed in this assignment. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Statistical Language Models\n",
    "\n",
    "### 1. Maximum likelihood estimation for statistical language models with proper smoothing (50pts)\n",
    "\n",
    "Use all the review documents to estimate a unigram language model $p(w)$ and two bigram language models (with different smoothing methods specified below). Note those language models are corpus-level models, i.e., aggregating all the words across different documents.\n",
    "\n",
    "When estimating the bigram language models, using linear interpolation smoothing and absolute discount smoothing based on the unigram language model $p_u(w)$ to get two different bigram language models accordingly, i.e., $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$. In linear interpolation smoothing, set the parameter $\\lambda=0.9$; and in absolute discount smoothing, set the parameter $\\delta=0.1$.\n",
    "\n",
    "Specifically, when estimating $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$, you should use the unigram language model $p(w_i)$ as the reference language model in smoothing. For example, in linear interpolation smoothing, the resulting smoothing formula looks like this,\n",
    "\n",
    "$$p^L(w_i|w_{i-1})=(1-\\lambda) \\frac{c(w_{i-1}w_i)}{c(w_{i-1})} + \\lambda p(w_i)$$ \n",
    "where $c(w_{i-1}w_i)$ is the frequency of bigram $w_{i-1}w_i$ in the whole corpus.\n",
    "\n",
    "From the resulting two bigram language models, find the top 10 words that are most likely to follow the word \"good\", i.e., rank the words in a descending order by $p^L(w|good\")$ and $p^A(w|good\")$ and output the top 10 words. Are those top 10 words the same from these two bigram language models? Explain your observation.\n",
    "\n",
    "*HINT: to reduce space complexity, you do not need to actually maintain a $V\\times V$ array to store the counts and probabilities for the bigram language models. You can use a sparse data structure, e.g., hash map, to store the seen words/bigrams, and perform the smoothing on the fly, i.e., evoke some function calls to return the value of $p^L(w|good\")$ and $p^A(w|good\")$.* \n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of the linear interpolation smoothing and absolute discount smoothing.\n",
    "2. The top 10 words selected from the corresponding two bigram language models.\n",
    "3. Your explanation of the observations about the top words under those two bigram language models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unigram language model p(w)\n",
    "list_of_tokenized_reviews = []\n",
    "with open('list_of_tokenized_reviews.pickle', 'rb') as file:\n",
    "    list_of_tokenized_reviews = pickle.load(file)\n",
    "token_freq_dict = total_term_frequency(list_of_tokenized_reviews)\n",
    "total_num_tokens = sum([len(tokens) for tokens in list_of_tokenized_reviews])\n",
    "token_prob_dict = {}\n",
    "for key in token_freq_dict.keys():\n",
    "    token_prob_dict[key] = token_freq_dict[key] / total_num_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the : 0.05306509260194806\n",
      "and : 0.03497929899421944\n",
      "i : 0.03195698201920177\n",
      "a : 0.02365215008387146\n",
      "to : 0.019990190189529227\n",
      "but : 0.01815881364406983\n",
      "it : 0.017903926404976605\n",
      "wa : 0.01704291139500162\n",
      "of : 0.014552406250610416\n",
      "for : 0.011796944880580106\n"
     ]
    }
   ],
   "source": [
    "#bigram language model with linear interpolation smoothing\n",
    "all_bigrams = get_all_bigrams(list_of_tokenized_reviews)\n",
    "bigram_freq_dict = total_bigram_freqency(all_bigrams)\n",
    "def p_linear(first, second):\n",
    "    lam = 0.9\n",
    "    try:\n",
    "        prob = (1-lam) * (bigram_freq_dict[(first, second)] / token_freq_dict[first]) + (lam * token_prob_dict[second])\n",
    "    except:\n",
    "        prob = (lam * token_prob_dict[second])\n",
    "    return prob\n",
    "#get the top most likely words to follow \"good\"\n",
    "linear_probabilities = {}\n",
    "for token in token_prob_dict.keys():\n",
    "    linear_probabilities[token] = p_linear('good', token)\n",
    "ans = dict(sorted(linear_probabilities.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('good', 'but') : 7560\n",
      "('good', 'and') : 5022\n",
      "('good', 'i') : 4455\n",
      "('good', 'the') : 4358\n",
      "('good', 'as') : 2874\n",
      "('good', 'food') : 2306\n",
      "('good', 'it') : 1507\n",
      "('good', 'thing') : 1403\n",
      "('good', 'for') : 1393\n",
      "('good', 'too') : 1385\n"
     ]
    }
   ],
   "source": [
    "#most popular words to follow 'good' with no smooting.\n",
    "target_bigrams = {}\n",
    "for bigram in bigram_freq_dict.keys():\n",
    "    if bigram[0] == 'good':\n",
    "        target_bigrams[bigram] = bigram_freq_dict[bigram]\n",
    "ans = dict(sorted(target_bigrams.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but : 0.09564301951999492\n",
      "and : 0.063624517412393\n",
      "i : 0.05644462810001931\n",
      "the : 0.05530186422700514\n",
      "as : 0.03636073506188902\n",
      "food : 0.029181046543558607\n",
      "it : 0.019120838648309255\n",
      "thing : 0.017747196904450483\n",
      "for : 0.01765550959445897\n",
      "too : 0.01752179260050426\n"
     ]
    }
   ],
   "source": [
    "#bigram language model with absolute discount smoothing\n",
    "def p_abs_disc(first, second):\n",
    "    delta = 0.1\n",
    "    d_u = len(target_bigrams) #number of unique bigrams with 'good' as the first word\n",
    "    try:\n",
    "        prob = (max(bigram_freq_dict[(first, second)] - delta, 0) + (delta * d_u * token_prob_dict[second])) / (token_freq_dict[first])\n",
    "    except:\n",
    "        prob = (delta * d_u * token_prob_dict[second]) / (token_freq_dict[first])\n",
    "    return prob\n",
    "\n",
    "abs_disc_probabilities = {}\n",
    "for token in token_prob_dict.keys():\n",
    "    abs_disc_probabilities[token] = p_abs_disc('good', token)\n",
    "ans = dict(sorted(abs_disc_probabilities.items(), key = lambda x: x[1], reverse = True)[:10])\n",
    "for line in ans:\n",
    "    print(f'{line} : {ans[line]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate text documents from a language model (40pts)\n",
    "\n",
    "Fixing the document length to 20, generate 10 documents by sampling words from $p(w)$, $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$ respectively.\n",
    "\n",
    "*HINT: you can use $p(w)$ to generate the first word of a document and then sampling from the corresponding bigram language model when generating from $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$.* \n",
    "\n",
    "**What to submit**:\n",
    "\n",
    "1. Paste your implementation of the sampling procedure from a language model.\n",
    "2. The 10 documents generated from $p(w)$, $p^L(w_i|w_{i-1})$ and $p^A(w_i|w_{i-1})$ accordingly, and the corresponding likelihood given by the used language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "doc_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "belli like becam i with also year someth sort spend it them dish wa student fantast we wo you is : 3.203829260978685e-58\n",
      "sinc there geog offer and dine better skip quit i hand burger wa the the broccoli into throw the pizza : 9.362810067602832e-60\n",
      "beef the around is up okay sport it neighborhood brought short tasti give onc part it angel did ve pack : 9.409861206274809e-61\n",
      "place becaus bar of or compens good lox the bar good and go sure idiot main along skirt with batch : 4.8775442722828e-62\n",
      "i sweetheart no think salad roll of that waitress food man flash companion a one i there serv be the : 1.7356431567417256e-56\n",
      "tini strictli littl it it no sport love me experi seri my with our delici to you those differ french : 2.764208419580971e-59\n",
      "the portion definit enough the him the by can miss get the would that me a chicken pizza mainten it : 1.1491056590870506e-51\n",
      "it on side st we big the here place sunday which to of previou mani definit for look had becaus : 9.92993180874294e-53\n",
      "henc around you natur small tax bld food uncomfort wait choic outsid pillow smokey goos spici so sangria a we : 4.051480967025093e-69\n",
      "to martini nighttim moment chocol good i rabbit just and and feel swine shout you soup you lemonad nois afternoon : 5.342396693249383e-66\n"
     ]
    }
   ],
   "source": [
    "#unigram\n",
    "unigram_tokens = []\n",
    "unigram_probs = []\n",
    "for key in token_prob_dict.keys():\n",
    "    unigram_tokens.append(key)\n",
    "    unigram_probs.append(token_prob_dict[key])\n",
    "for i in range(10):\n",
    "    samples = np.random.choice(unigram_tokens, doc_size, p=unigram_probs)\n",
    "    final_probability = token_prob_dict[samples[0]]\n",
    "    for i in range(1, len(samples)):\n",
    "        final_probability *= token_prob_dict[samples[i]]\n",
    "    print(' '.join(samples)+ ' : ' + str(final_probability))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "appet woodi s ok the the would is while flavor the is due just sure find ani s the rosco : 9.423746067339852e-54\n",
      "both am dine thi look for crust salmon themselv and is of it the thi how crowd go came a : 3.325664595975003e-51\n",
      "the ve mind team eaten hype than veri flavori great notch also nt as into then so a restaur a : 3.618785728977104e-60\n",
      "a sandwich and loiter cobbler the a someplac her and two the over as their will tabl calamari nt wa : 2.6663383519321672e-55\n",
      "we all you can you wa beer whi in thi nice great food bit wa place of atmospher 30pm beef : 2.5805764312122374e-50\n",
      "campbel wa tast wa veri were not is sweet better for nt disappoint i hear food but if drum the : 6.672544242351564e-53\n",
      "as the dessert tast perfectli is select hot take good can the my and tabl they pretti the tongu to : 2.5363192214949507e-50\n",
      "lo squid a also hypnot lesson fish when wait plate spanish and made and the would prefer beehiv not are : 2.4111723100255136e-62\n",
      "urin kitschi milk mayb first nt the e tasti is and the ask a littl a and surf wine in : 1.3972220099369338e-57\n",
      "were write at where we two friend so i read it peopl our it two hypochondriac but far made hear : 2.9284970292559926e-58\n"
     ]
    }
   ],
   "source": [
    "#bigram linear interp\n",
    "\n",
    "for i in range(10):\n",
    "    samples = []\n",
    "    prev_word = np.random.choice(unigram_tokens, 1, p=unigram_probs)[0]\n",
    "    samples.append(prev_word)\n",
    "    final_probability = token_prob_dict[prev_word]\n",
    "    for i in range(doc_size-1):\n",
    "        lin_int_probs_dict = {}\n",
    "        for token in token_prob_dict.keys():\n",
    "            lin_int_probs_dict[token] = p_linear(prev_word, token)\n",
    "        lin_int_tokens = []\n",
    "        lin_int_probs = []\n",
    "        for key in lin_int_probs_dict.keys():\n",
    "            lin_int_tokens.append(key)\n",
    "            lin_int_probs.append(lin_int_probs_dict[key])\n",
    "        lin_int_probs = np.array(lin_int_probs)\n",
    "        lin_int_probs /= sum(lin_int_probs)\n",
    "        prev_word = np.random.choice(lin_int_tokens, 1, p=lin_int_probs)[0]\n",
    "\n",
    "        samples.append(prev_word)\n",
    "        final_probability *= lin_int_probs_dict[prev_word]\n",
    "        \n",
    "    print(' '.join(samples) + ' : ' + str(final_probability))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oliv oil yum kun food and you ve been disappoint articl stupend ca nt big and the mussel food prep : 1.4779190893089667e-47\n",
      "NUM pitcher or i enjoy the front of brisket is incred disappoint dippin pick bachi burger well season and realli : 3.036954549756277e-43\n",
      "breaker diminish the wait NUM NUM star for NUM month later we start on the parmesan crust overal review fantast : 2.877254956175923e-43\n",
      "empti threat price for an old pie but nope that the wall sign leg say except amount of the artichok : 5.997603303385795e-51\n",
      "from the squash ravioli and follow the latk failb franki a night wa nt come back to eat where you : 1.7073004860111123e-39\n",
      "patron a young to becom a lot of thing to expect had pizza wa pretti fun to go to eat : 1.28968800792104e-41\n",
      "be a scale goe my recommend from what time they are much but the eggplant anoth parti or simpli delici : 1.8326197847967852e-44\n",
      "look over ambianc fantast swank of them blah blah quit small the food to go away thi a wide eye : 5.131260330517393e-47\n",
      "onli for dessert apart over for a poboy a saturday we had and sweet but she later i wa also : 4.931408943833155e-44\n",
      "catch sport with a rel quickli we were not the burger though tast bud along got in their taco so : 3.680132593706084e-45\n"
     ]
    }
   ],
   "source": [
    "#bigram absolute discount\n",
    "for i in range(10):\n",
    "    samples = []\n",
    "    prev_word = np.random.choice(unigram_tokens, 1, p=unigram_probs)[0]\n",
    "    samples.append(prev_word)\n",
    "    final_probability = token_prob_dict[prev_word]\n",
    "    for i in range(doc_size-1):\n",
    "        abs_disc_probs_dict = {}\n",
    "        for token in token_prob_dict.keys():\n",
    "            abs_disc_probs_dict[token] = p_abs_disc(prev_word, token)\n",
    "        abs_disc_tokens = []\n",
    "        abs_disc_probs = []\n",
    "        for key in abs_disc_probs_dict.keys():\n",
    "            abs_disc_tokens.append(key)\n",
    "            abs_disc_probs.append(abs_disc_probs_dict[key])\n",
    "        abs_disc_probs = np.array(abs_disc_probs)\n",
    "        abs_disc_probs /= sum(abs_disc_probs)\n",
    "        prev_word = np.random.choice(abs_disc_tokens, 1, p=abs_disc_probs)[0]\n",
    "        samples.append(prev_word)\n",
    "        final_probability *= abs_disc_probs_dict[prev_word]\n",
    "    print(' '.join(samples) + ' : ' + str(final_probability))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment — Belief or Bias in Information Retrieval (10pts)\n",
    "In our class, we have learned both classical and modern information retrieval evaluation methods. And their shared goal is to assess if a retrieval system can satisfy users' information need. Such an evaluation directly leads to the subsequent optimization of retrieval system, e.g., optimize the ranking for click-through rates. But should a system please its users so as to improve the metrics or should it educate the users about what is right and wrong?\n",
    "\n",
    "Let's read the paper [\"Beliefs and biases in web search\"](https://dl.acm.org/doi/10.1145/2484028.2484053), which is the best paper in SIGIR'2013. Based on the findings of this paper and current public concern/debate of the wide spread of misinformation on the web, what kind of suggestion do you want to give to Google and Bing to improve the situation? You can focus on the search evaluation, document retrieval and ranking, or any aspect related to the retrieval process."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Credits (5pts)\n",
    "\n",
    "You are encouraged to further investigate the relation between classic language model and the trending Large Language Models. How LLMs differ from unigram and bigram models we implemented? It is okay to consult LLMs for this question :\\) \n",
    "\n",
    "# Submission\n",
    "\n",
    "This assignment has in total 100 points. The deadline is Feb 20 23:59 PDT. You should submit your report in **PDF** using the homework latex template, and submit your code (notebook)。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
